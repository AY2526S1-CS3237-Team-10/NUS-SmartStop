{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b9ed16",
   "metadata": {},
   "source": [
    "# Training CNN for crowd level classification\n",
    "\n",
    "The model takes in pictures of the bus stop, classified into their respective crowd levels.\n",
    "These classified images then is fed into the model to train. The output model can then be used to classify the existing state of the bus stop.\n",
    "\n",
    "Folder Structure:\n",
    "```\n",
    "/home/root/my_project_training/\n",
    "│\n",
    "├── 0_empty/\n",
    "│   ├── empty_01.jpg\n",
    "│   ├── empty_02.jpg\n",
    "│   └── ...\n",
    "│\n",
    "├── 1_low/\n",
    "│   ├── 1_low_01.jpg\n",
    "│   └── ...\n",
    "│\n",
    "└── 2_medium/\n",
    "    ├── 2_medium_01.jpg\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93117d85",
   "metadata": {},
   "source": [
    "Install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd9369d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (0.24.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (2.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jun xu\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5cda0",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "711eb62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys # Added sys for exiting\n",
    "\n",
    "# --- Define Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359fd81",
   "metadata": {},
   "source": [
    "Define Data Transforms and Load Data\n",
    "\n",
    "Add black bars to make it a square to enable transfer learning, MobileNetV2 needs square input.\n",
    "\n",
    "Use data augmentation to create variations on the same dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5359824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transform to \"letterbox\" (pad) the image to a square\n",
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        max_wh = max(w, h)\n",
    "        hp = int((max_wh - w) / 2)\n",
    "        vp = int((max_wh - h) / 2)\n",
    "        padding = (hp, vp, hp, vp)\n",
    "        return transforms.functional.pad(image, padding, 0, 'constant')\n",
    "\n",
    "# Create a separate transform pipeline for TRAINING ---\n",
    "# This includes augmentations to create more varied data\n",
    "train_transform = transforms.Compose([\n",
    "    SquarePad(),                      # 1. Pad to be a square\n",
    "    transforms.Resize((224, 224)),    # 2. Shrink to 224x224\n",
    "    # --- NEW Augmentations ---\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # 3. Randomly flip images horizontally\n",
    "    transforms.RandomRotation(10),          # 4. Randomly rotate by up to 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 5. Tweak colors\n",
    "    # --- End Augmentations ---\n",
    "    transforms.ToTensor(),            # 6. Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 7. Normalize\n",
    "])\n",
    "\n",
    "# Create a separate transform pipeline for VALIDATION/TESTING ---\n",
    "# This has NO augmentation, just the required preprocessing\n",
    "val_transform = transforms.Compose([\n",
    "    SquarePad(),                      # 1. Pad to be a square\n",
    "    transforms.Resize((224, 224)),    # 2. Shrink to 224x224\n",
    "    transforms.ToTensor(),            # 3. Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 4. Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549f95d",
   "metadata": {},
   "source": [
    "Load image data for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all data from: ./test_train_esp32cam_smartstop/\n"
     ]
    }
   ],
   "source": [
    "# --- Load Your Custom Data from Folders ---\n",
    "# This is the PyTorch version of \"image_dataset_from_directory\"\n",
    "# data_dir = '/home/root/my_project_training/'\n",
    "data_dir = './train_combined/'\n",
    "print(f\"Loading all data from: {data_dir}\")\n",
    "\n",
    "# Load the dataset TWICE, once with each transform pipeline\n",
    "# This is necessary so we can assign the correct transforms to the train/val splits\n",
    "try:\n",
    "    # Dataset for TRAINING (will be subset-ted later)\n",
    "    full_dataset_train = datasets.ImageFolder(\n",
    "        root=data_dir,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    # Dataset for VALIDATION (will be subset-ted later)\n",
    "    # We use this one to get the class names and perform the split\n",
    "    full_dataset_val = datasets.ImageFolder(\n",
    "        root=data_dir,\n",
    "        transform=val_transform\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data directory not found at {data_dir}\")\n",
    "    print(\"Please update the 'data_dir' variable to point to your dataset.\")\n",
    "    sys.exit(1)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: No valid images found in {data_dir}. Check the path and folder structure. Details: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9bca5",
   "metadata": {},
   "source": [
    "Create Data Loaders & Get Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3984d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 classes: ['0_empty', '1_low', '2_medium', '3_high']\n",
      "Total images: 140. Splitting into 112 train, 28 validation.\n",
      "Calculating class weights to handle imbalance...\n",
      "Class counts: [('0_empty', np.int64(40)), ('1_low', np.int64(29)), ('2_medium', np.int64(18)), ('3_high', np.int64(25))]\n",
      "Calculated weights: [0.7       0.9655172 1.5555556 1.12     ]\n"
     ]
    }
   ],
   "source": [
    "# Automatically finds your folders: e.g., ['0_empty', '1_low', '2_medium']\n",
    "class_names = full_dataset_val.classes\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(f\"Found {NUM_CLASSES} classes: {class_names}\")\n",
    "\n",
    "\n",
    "# --- Split the Data ---\n",
    "validation_split = 0.2 # e.g., 20% for validation\n",
    "total_size = len(full_dataset_val)\n",
    "val_size = int(total_size * validation_split)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "if train_size <= 0 or val_size <= 0:\n",
    "    print(f\"Error: Dataset is too small to split (Total: {total_size}). Need more images.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "    \n",
    "print(f\"Total images: {total_size}. Splitting into {train_size} train, {val_size} validation.\")\n",
    "\n",
    "# Perform the random split ONCE to get the indices\n",
    "# We use the val_dataset instance just to perform the split, but the indices are universal\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_subset_indices, val_subset_indices = random_split(\n",
    "    range(total_size), # Split on the *indices*\n",
    "    [train_size, val_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "# --- Apply the splits to the correct datasets ---\n",
    "# Create the final datasets using the indices from the split\n",
    "# The train_dataset now points to the augmented data\n",
    "train_dataset = Subset(full_dataset_train, train_subset_indices)\n",
    "# The validation_dataset points to the non-augmented data\n",
    "validation_dataset = Subset(full_dataset_val, val_subset_indices)\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True # IMPORTANT: Shuffle training data\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # No need to shuffle validation/test data\n",
    ")\n",
    "\n",
    "\n",
    "# --- Calculate Class Weights for Imbalanced Data ---\n",
    "print(\"Calculating class weights to handle imbalance...\")\n",
    "\n",
    "# Get all labels from the full dataset\n",
    "all_labels = full_dataset_val.targets # Get all labels from the master list\n",
    "# Get the labels just for our training split\n",
    "train_indices = train_subset_indices.indices # Get the list of indices\n",
    "labels_list = [all_labels[i] for i in train_indices]\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = np.bincount(labels_list)\n",
    "print(f\"Class counts: {list(zip(class_names, class_counts))}\")\n",
    "\n",
    "# Calculate weights (inverse frequency)\n",
    "# Formula: total_samples / (num_classes * class_count)\n",
    "total_samples = len(labels_list)\n",
    "# Add epsilon (1e-6) to prevent division by zero if a class has 0 samples in split\n",
    "class_weights = total_samples / (NUM_CLASSES * torch.tensor(class_counts, dtype=torch.float32) + 1e-6)\n",
    "\n",
    "print(f\"Calculated weights: {class_weights.numpy()}\")\n",
    "\n",
    "# Move weights to the correct device\n",
    "class_weights = class_weights.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c639b7a",
   "metadata": {},
   "source": [
    "Define the Model (Transfer Learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30e0fb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load Pre-trained MobileNetV2 ---\n",
    "# weights='DEFAULT' loads the best available pre-trained weights\n",
    "model = torchvision.models.mobilenet_v2(weights='DEFAULT')\n",
    "\n",
    "# --- Freeze the \"Feature Extractor\" ---\n",
    "# This prevents wrecking the pre-trained \"expert\" knowledge\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- Replace the \"Classifier Head\" ---\n",
    "# MobileNetV2's last layer is called 'classifier'. We replace it with a new one\n",
    "# that fits the number of classes (e.g., 4 instead of 1000).\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2),            # Dropout to prevent overfitting on your small dataset\n",
    "    nn.Linear(model.last_channel, NUM_CLASSES) # The new 4-class output layer\n",
    ")\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f49858",
   "metadata": {},
   "source": [
    "Training Setup & Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10e9df18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/50 - Train Loss: 1.3312 - Val Loss: 1.2638 - Val Acc: 0.5714\n",
      "Epoch 2/50 - Train Loss: 1.1312 - Val Loss: 1.1546 - Val Acc: 0.6071\n",
      "Epoch 3/50 - Train Loss: 1.0437 - Val Loss: 1.0541 - Val Acc: 0.7143\n",
      "Epoch 4/50 - Train Loss: 0.9030 - Val Loss: 0.9913 - Val Acc: 0.7857\n",
      "Epoch 5/50 - Train Loss: 0.8488 - Val Loss: 0.9646 - Val Acc: 0.7500\n",
      "Epoch 6/50 - Train Loss: 0.7648 - Val Loss: 0.8859 - Val Acc: 0.7857\n",
      "Epoch 7/50 - Train Loss: 0.6781 - Val Loss: 0.8458 - Val Acc: 0.7857\n",
      "Epoch 8/50 - Train Loss: 0.6522 - Val Loss: 0.7900 - Val Acc: 0.8214\n",
      "Epoch 9/50 - Train Loss: 0.6093 - Val Loss: 0.7536 - Val Acc: 0.8214\n",
      "Epoch 10/50 - Train Loss: 0.5895 - Val Loss: 0.7193 - Val Acc: 0.8571\n",
      "Epoch 11/50 - Train Loss: 0.6178 - Val Loss: 0.7093 - Val Acc: 0.8571\n",
      "Epoch 12/50 - Train Loss: 0.5832 - Val Loss: 0.7007 - Val Acc: 0.8571\n",
      "Epoch 13/50 - Train Loss: 0.4816 - Val Loss: 0.6994 - Val Acc: 0.8214\n",
      "Epoch 14/50 - Train Loss: 0.5499 - Val Loss: 0.6842 - Val Acc: 0.8571\n",
      "Epoch 15/50 - Train Loss: 0.4340 - Val Loss: 0.6737 - Val Acc: 0.8571\n",
      "Epoch 16/50 - Train Loss: 0.4517 - Val Loss: 0.6837 - Val Acc: 0.8214\n",
      "Epoch 17/50 - Train Loss: 0.4214 - Val Loss: 0.6259 - Val Acc: 0.8214\n",
      "Epoch 18/50 - Train Loss: 0.4622 - Val Loss: 0.6129 - Val Acc: 0.8214\n",
      "Epoch 19/50 - Train Loss: 0.3850 - Val Loss: 0.5998 - Val Acc: 0.8214\n",
      "Epoch 20/50 - Train Loss: 0.3609 - Val Loss: 0.6131 - Val Acc: 0.8214\n",
      "Epoch 21/50 - Train Loss: 0.3971 - Val Loss: 0.6036 - Val Acc: 0.8571\n",
      "Epoch 22/50 - Train Loss: 0.3691 - Val Loss: 0.5857 - Val Acc: 0.8214\n",
      "Epoch 23/50 - Train Loss: 0.3757 - Val Loss: 0.5764 - Val Acc: 0.8214\n",
      "Epoch 24/50 - Train Loss: 0.3619 - Val Loss: 0.5621 - Val Acc: 0.8214\n",
      "Epoch 25/50 - Train Loss: 0.3842 - Val Loss: 0.5650 - Val Acc: 0.8214\n",
      "Epoch 26/50 - Train Loss: 0.3270 - Val Loss: 0.5790 - Val Acc: 0.8214\n",
      "Epoch 27/50 - Train Loss: 0.3379 - Val Loss: 0.5553 - Val Acc: 0.8571\n",
      "Epoch 28/50 - Train Loss: 0.3677 - Val Loss: 0.5686 - Val Acc: 0.8214\n",
      "Epoch 29/50 - Train Loss: 0.3535 - Val Loss: 0.5428 - Val Acc: 0.8571\n",
      "Epoch 30/50 - Train Loss: 0.3598 - Val Loss: 0.5400 - Val Acc: 0.8571\n",
      "Epoch 31/50 - Train Loss: 0.3089 - Val Loss: 0.5447 - Val Acc: 0.8214\n",
      "Epoch 32/50 - Train Loss: 0.3674 - Val Loss: 0.5443 - Val Acc: 0.8571\n",
      "Epoch 33/50 - Train Loss: 0.2763 - Val Loss: 0.5548 - Val Acc: 0.8571\n",
      "Epoch 34/50 - Train Loss: 0.3510 - Val Loss: 0.5644 - Val Acc: 0.8571\n",
      "Epoch 35/50 - Train Loss: 0.2914 - Val Loss: 0.5496 - Val Acc: 0.8571\n",
      "Epoch 36/50 - Train Loss: 0.3555 - Val Loss: 0.5489 - Val Acc: 0.8571\n",
      "Epoch 37/50 - Train Loss: 0.2854 - Val Loss: 0.5317 - Val Acc: 0.8571\n",
      "Epoch 38/50 - Train Loss: 0.3125 - Val Loss: 0.5345 - Val Acc: 0.8571\n",
      "Epoch 39/50 - Train Loss: 0.3339 - Val Loss: 0.5170 - Val Acc: 0.8571\n",
      "Epoch 40/50 - Train Loss: 0.2755 - Val Loss: 0.5198 - Val Acc: 0.8571\n",
      "Epoch 41/50 - Train Loss: 0.2822 - Val Loss: 0.5354 - Val Acc: 0.8571\n",
      "Epoch 42/50 - Train Loss: 0.2943 - Val Loss: 0.5271 - Val Acc: 0.8571\n",
      "Epoch 43/50 - Train Loss: 0.2971 - Val Loss: 0.5145 - Val Acc: 0.8571\n",
      "Epoch 44/50 - Train Loss: 0.2748 - Val Loss: 0.5310 - Val Acc: 0.8571\n",
      "Epoch 45/50 - Train Loss: 0.2777 - Val Loss: 0.5049 - Val Acc: 0.8571\n",
      "Epoch 46/50 - Train Loss: 0.2831 - Val Loss: 0.5151 - Val Acc: 0.8571\n",
      "Epoch 47/50 - Train Loss: 0.2735 - Val Loss: 0.5148 - Val Acc: 0.8571\n",
      "Epoch 48/50 - Train Loss: 0.3036 - Val Loss: 0.5369 - Val Acc: 0.8571\n",
      "Epoch 49/50 - Train Loss: 0.2569 - Val Loss: 0.5098 - Val Acc: 0.8571\n",
      "Epoch 50/50 - Train Loss: 0.3204 - Val Loss: 0.5185 - Val Acc: 0.8571\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# --- Define Hyperparameters ---\n",
    "lr = 0.001\n",
    "num_epochs = 50 # 20 epochs is usually enough for transfer learning\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# Only optimize parameters that require gradients (your new head)\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_val_loss = val_loss / len(test_loader)\n",
    "    val_acc = val_correct.double() / len(validation_dataset)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {epoch_val_loss:.4f} - Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(epoch_val_loss)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc896d1d",
   "metadata": {},
   "source": [
    "Save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "228b989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'smartstop_mobilenet_v2_esp32cam_smartstop.pth'\n",
      "Class names saved to 'class_names.json'\n"
     ]
    }
   ],
   "source": [
    "# --- Save the Model ---\n",
    "# It's best practice to save the 'state_dict' (just the weights)\n",
    "torch.save(model.state_dict(), 'smartstop_mobilenet_v2_esp32cam_smartstop.pth')\n",
    "print(\"Model saved as 'smartstop_mobilenet_v2_esp32cam_smartstop.pth'\")\n",
    "\n",
    "# Save the class names too, need them for the Flask app\n",
    "import json\n",
    "with open('class_names.json', 'w') as f:\n",
    "    json.dump(class_names, f)\n",
    "print(\"Class names saved to 'class_names.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
